# üß™ Nook Typewriter Test Suite Documentation

*Generated: December 13, 2024*

## Overview

The Nook Typewriter project employs a comprehensive test suite with 32 test scripts covering critical functionality, ensuring system reliability within the constrained embedded environment.

---

## üìä Test Suite Metrics

| Metric | Value |
|--------|-------|
| **Total Test Scripts** | 32 |
| **Test Categories** | 10 |
| **Coverage Target** | 90% |
| **Priority Levels** | High, Medium, Low |
| **Test Functions** | 58+ |
| **Execution Time** | ~5 minutes |

---

## üèóÔ∏è Test Framework Architecture

### Core Framework (`test-framework.sh`)

**Purpose**: Provides common testing utilities and assertions for all unit tests

**Key Components**:
```bash
# Test initialization
init_test(name)          # Initialize test with name
pass_test(message)        # Mark test as passed
fail_test(message)        # Mark test as failed
skip_test(reason)         # Skip test with reason

# Assertions
assert_equals(expected, actual, message)
assert_not_empty(value, message)
assert_file_exists(path)
assert_command_exists(command)
assert_memory_under(limit)

# Utilities
require_root()            # Check for root privileges
require_docker()          # Check Docker availability
require_device()          # Check for Nook hardware
```

**Environment Variables**:
- `TEST_ROOT` - Test directory path
- `PROJECT_ROOT` - Project root directory
- `TEST_NAME` - Current test name
- `TEST_PASSED` - Pass counter
- `TEST_FAILED` - Fail counter

### Master Test Runner (`run-all-tests.sh`)

**Purpose**: Orchestrates all test execution and generates reports

**Execution Flow**:
1. Initialize test environment
2. Create report directory
3. Execute tests by category
4. Collect results
5. Generate markdown report
6. Display summary

**Report Generation**:
- Location: `tests/reports/test_report_[timestamp].md`
- Log file: `tests/reports/test_log_[timestamp].txt`
- Includes: Pass/fail counts, execution time, failed test list

---

## üìÅ Test Categories

### 1. **Toolchain Tests** (`/unit/toolchain/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-cross-compiler.sh` | Verify ARM cross-compiler availability | High |
| `test-docker-image.sh` | Validate Docker build environment | High |
| `test-ndk-version.sh` | Check Android NDK version (r10e) | High |

**Key Validations**:
- Cross-compiler: `arm-linux-androideabi-gcc`
- Docker image: `jokernel-builder`
- NDK path: Proper environment setup

### 2. **Kernel Tests** (`/unit/kernel/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-kernel-integration.sh` | Verify kernel build integration | High |
| `test-jesteros-core.sh` | Test core module structure | High |
| `test-jester-module.sh` | Validate jester module | Medium |
| `test-typewriter-module.sh` | Test statistics tracking | Medium |
| `test-wisdom-module.sh` | Verify quotes module | Low |

**Module Tests Check**:
- Source file existence
- Proper includes and headers
- Module init/exit functions
- /proc entry creation code
- Kernel config integration

### 3. **Memory Tests** (`/unit/memory/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-kernel-memory-config.sh` | Verify memory constraints | High |
| `test-docker-memory-limit.sh` | Test container limits | High |

**Memory Constraints Verified**:
- OS limit: <96MB
- Sacred writing space: 160MB preserved
- Docker container limits
- Kernel config memory settings

### 4. **Boot Tests** (`/unit/boot/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-boot-scripts-exist.sh` | Verify boot script presence | High |
| `test-module-loading-sequence.sh` | Test module load order | High |

**Boot Sequence Validation**:
1. Boot scripts present in `/usr/local/bin/`
2. Module loading order: core ‚Üí jester ‚Üí typewriter ‚Üí wisdom
3. Init script execution
4. Jester display at boot

### 5. **Menu Tests** (`/unit/menu/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-menu-exists.sh` | Verify menu scripts present | Medium |
| `test-menu-input-validation.sh` | Test input sanitization | High |
| `test-menu-error-handling.sh` | Validate error handling | High |

**Menu System Checks**:
- Script location and permissions
- Input validation functions
- Error handler implementation
- E-Ink display abstraction

### 6. **E-Ink Tests** (`/unit/eink/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-display-abstraction.sh` | Test display fallback | Medium |

**Display Validation**:
- FBInk command detection
- Terminal fallback mechanism
- Display clearing functions
- Text positioning

### 7. **Theme Tests** (`/unit/theme/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-jester-ascii-art.sh` | Verify ASCII art files | Low |
| `test-medieval-messages.sh` | Check theme consistency | Low |

**Theme Validation**:
- ASCII art file presence
- Jester mood variations
- Medieval terminology usage
- Message formatting

### 8. **Module Build Tests** (`/unit/modules/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-module-sources.sh` | Verify source files | High |
| `test-module-kconfig.sh` | Test Kconfig entries | High |
| `test-module-makefile.sh` | Validate Makefile | High |

**Build System Checks**:
- Kconfig.jesteros presence
- Makefile rules
- Source file structure
- Header dependencies

### 9. **Documentation Tests** (`/unit/docs/`)

| Test Script | Purpose | Priority |
|-------------|---------|----------|
| `test-xda-research-docs.sh` | Verify XDA documentation | Low |

### 10. **Build Tests** (`/unit/build/`)

Build verification tests (directory present but tests pending)

---

## üéØ Priority Test Suites

### High Priority Suite (`test-high-priority.sh`)

**Coverage**: Critical system functionality

**Test Functions**:
```bash
test_toolchain_available()       # Verify build tools
test_docker_environment()         # Check Docker setup
test_memory_constraints()         # Validate RAM limits
test_kernel_modules_exist()       # Check module sources
test_boot_sequence()              # Verify boot scripts
test_menu_input_validation()      # Test input safety
```

**Execution**: Must pass 100% for release

### Medium Priority Suite (`test-medium-priority.sh`)

**Coverage**: Important but non-critical features

**Test Functions**:
```bash
test_menu_system()                # Menu functionality
test_ascii_art_resources()        # Theme assets
test_vim_configuration()          # Editor setup
test_e_ink_abstraction()          # Display handling
test_service_scripts()            # Background services
```

**Execution**: Target 85% pass rate

### UI Components Suite (`test-ui-components.sh`)

**Coverage**: User interface elements

**Test Functions**:
```bash
test_display_component()          # Display abstraction
test_menu_component()             # Menu system
test_jester_display()             # Jester rendering
test_progress_indicators()        # Progress display
test_error_messages()             # Error formatting
```

---

## üîß Test Execution

### Running All Tests
```bash
cd /home/jyeary/projects/personal/nook/tests
./run-all-tests.sh
```

### Running Category Tests
```bash
# Run specific category
./run-all-tests.sh unit/kernel

# Run priority suite
./test-high-priority.sh

# Run individual test
./unit/kernel/test-jesteros-core.sh
```

### Test Output Format
```
[TEST] Running: test_name
[PASS] test_name
[FAIL] test_name: reason
[SKIP] test_name: dependency not met
```

### Report Format
```markdown
# Test Report - [timestamp]

## Summary
- Total Tests: N
- Passed: N (X%)
- Failed: N
- Skipped: N

## Failed Tests
- test_name: failure reason

## Test Details
[Detailed results per category]
```

---

## üìà Coverage Analysis

### Current Coverage

| Component | Coverage | Target | Status |
|-----------|----------|--------|--------|
| Kernel Modules | 95% | 90% | ‚úÖ |
| Boot System | 90% | 90% | ‚úÖ |
| Menu System | 85% | 80% | ‚úÖ |
| Memory Management | 100% | 100% | ‚úÖ |
| E-Ink Display | 70% | 70% | ‚úÖ |
| Theme/ASCII | 60% | 50% | ‚úÖ |
| Build System | 80% | 85% | ‚ö†Ô∏è |

### Coverage Gaps

**Needs Additional Testing**:
- Hardware-specific functions (requires device)
- SD card operations
- Power management
- Long-running stability

**Cannot Test in CI**:
- E-Ink display output
- Physical button input
- Battery monitoring
- USB keyboard support

---

## üö® Test Safety Features

### Input Validation Tests
- Path traversal prevention
- Command injection protection
- Buffer overflow checks
- Integer overflow validation

### Memory Safety Tests
- Memory leak detection
- Stack overflow prevention
- Heap corruption checks
- Resource cleanup validation

### Error Handling Tests
- Graceful failure modes
- Error message clarity
- Recovery mechanisms
- Logging completeness

---

## üîÑ Continuous Integration

### Pre-Commit Hooks
```bash
# Run before each commit
./tests/test-high-priority.sh || exit 1
```

### Build Verification
```bash
# Run after build
./tests/unit/kernel/test-kernel-integration.sh
./tests/unit/modules/test-module-sources.sh
```

### Release Validation
```bash
# Full suite before release
./tests/run-all-tests.sh
# Must achieve 90% pass rate
```

---

## üìù Writing New Tests

### Test Template
```bash
#!/bin/bash
# Test: [Component Name]
# Purpose: [What this tests]

source "$(dirname "$0")/../../test-framework.sh"

init_test "test_component_feature"

# Test implementation
if [[ -f "/expected/path" ]]; then
    pass_test "Component feature works"
else
    fail_test "Component feature missing"
fi
```

### Best Practices
1. **One assertion per test** - Keep tests focused
2. **Clear naming** - test_component_feature format
3. **Cleanup** - Always cleanup test artifacts
4. **Independence** - Tests shouldn't depend on each other
5. **Documentation** - Comment complex assertions

### Categories for New Tests
- Place in appropriate `/unit/[category]/` directory
- Update category in `run-all-tests.sh`
- Add to priority suite if critical
- Document in this file

---

## üêõ Common Test Failures

### Issue: Docker not available
**Solution**: Install Docker or skip with `SKIP_DOCKER=1`

### Issue: Missing cross-compiler
**Solution**: Install ARM toolchain or use Docker environment

### Issue: Permission denied
**Solution**: Some tests require root for mount operations

### Issue: Memory test fails
**Solution**: Check system has sufficient RAM available

---

## üìä Test Metrics History

### Recent Test Runs
| Date | Total | Passed | Failed | Coverage |
|------|-------|--------|--------|----------|
| 2024-12-13 | 32 | 28 | 2 | 87.5% |
| 2024-12-12 | 30 | 27 | 1 | 90.0% |
| 2024-12-11 | 28 | 25 | 1 | 89.3% |

### Improvement Trends
- Memory tests: 100% stable
- Module tests: Improved from 70% to 95%
- UI tests: New category at 80% coverage

---

## üîó Related Documentation

- [Test Framework Guide](test-framework-guide.md)
- [CI/CD Pipeline](ci-cd-setup.md)
- [Contributing Tests](contributing.md#tests)
- [Coverage Reports](reports/)

---

*"Test early, test often, jest eternally!"* üé≠

**Version**: 1.0.0  
**Last Updated**: December 13, 2024  
**Maintainer**: JoKernel Test Team# üß™ Test Framework Quick Reference

*Essential reference for QuillKernel test development*

## Framework Architecture

```
tests/
‚îú‚îÄ‚îÄ test-framework.sh          # Core framework
‚îú‚îÄ‚îÄ run-all-tests.sh           # Master test runner
‚îú‚îÄ‚îÄ test-improvements.sh       # Safety validation
‚îú‚îÄ‚îÄ test-high-priority.sh      # Critical tests
‚îú‚îÄ‚îÄ test-medium-priority.sh    # Standard tests
‚îî‚îÄ‚îÄ unit/                      # Unit test suites
    ‚îú‚îÄ‚îÄ boot/                  # Boot sequence tests
    ‚îú‚îÄ‚îÄ memory/                # Memory constraint tests
    ‚îú‚îÄ‚îÄ modules/               # Kernel module tests
    ‚îú‚îÄ‚îÄ menu/                  # UI and menu tests
    ‚îú‚îÄ‚îÄ toolchain/             # Build environment tests
    ‚îî‚îÄ‚îÄ theme/                 # Medieval theme tests
```

---

## Core Functions

### Test Lifecycle
```bash
init_test "Test Name"          # Initialize test with name
pass_test "Success message"    # Mark test as passed and exit
fail_test "Failure message"    # Mark test as failed and exit
skip_test "Skip reason"        # Skip test with reason and exit
```

### Basic Assertions
```bash
assert_equals "expected" "actual" "message"
assert_not_equals "unexpected" "actual" "message"
assert_contains "haystack" "needle" "message"
assert_not_contains "haystack" "needle" "message"
```

### File System Assertions
```bash
assert_file_exists "/path/to/file" "message"
assert_file_not_exists "/path/to/file" "message"
assert_directory_exists "/path/to/dir" "message"
assert_executable "/path/to/script" "message"
```

### Numeric Assertions
```bash
assert_greater_than "$value" "$threshold" "message"
assert_less_than "$value" "$threshold" "message"
```

### System Assertions
```bash
assert_command_exists "command_name" "message"
assert_docker_image_exists "image_name" "message"
```

---

## Standard Test Template

```bash
#!/bin/bash
# Unit Test: [Brief Description]
# [Detailed explanation of test purpose and scope]

set -euo pipefail

source "$(dirname "$0")/../../test-framework.sh"

init_test "[Test Display Name]"

# Constants (use readonly for immutable values)
readonly EXPECTED_FILE="$PROJECT_ROOT/path/to/file"
readonly -a REQUIRED_ITEMS=("item1" "item2" "item3")

# Test configuration
TEST_TIMEOUT=30
MEMORY_LIMIT_MB=256

# Setup function (optional)
setup_test() {
    # Prepare test environment
    mkdir -p /tmp/test_$$
    export TEST_DIR="/tmp/test_$$"
}

# Cleanup function (optional)  
cleanup() {
    # Clean up test artifacts
    rm -rf "$TEST_DIR" 2>/dev/null || true
}

# Register cleanup
trap cleanup EXIT

# Main test logic
setup_test

# Example validation
assert_file_exists "$EXPECTED_FILE" "Required file should exist"

# Success path
pass_test "All validations completed successfully"
```

---

## Enhanced Test Patterns

### Memory-Aware Testing
```bash
test_memory_constraint() {
    local max_memory_mb=96
    local process_name="$1"
    
    echo "  Testing memory usage for: $process_name"
    
    # Monitor memory usage
    local mem_usage
    mem_usage=$(docker run --rm -m "${max_memory_mb}m" "$process_name" \
        bash -c 'free -m | awk "/^Mem:/ {print \$3}"' 2>/dev/null || echo "0")
    
    assert_less_than "$mem_usage" "$max_memory_mb" \
        "Memory usage should be under ${max_memory_mb}MB"
    
    echo "  ‚úì Memory usage: ${mem_usage}MB (limit: ${max_memory_mb}MB)"
}
```

### Multi-File Validation
```bash
test_file_collection() {
    local base_dir="$1"
    local -a required_files=("${@:2}")
    
    local found_count=0
    local missing_files=()
    
    for file in "${required_files[@]}"; do
        local full_path="$base_dir/$file"
        if [[ -f "$full_path" ]]; then
            echo "  ‚úì Found: $file"
            ((found_count++))
        else
            echo "  ‚úó Missing: $file"
            missing_files+=("$file")
        fi
    done
    
    if [[ $found_count -eq ${#required_files[@]} ]]; then
        pass_test "All required files present ($found_count/${#required_files[@]})"
    elif [[ $found_count -gt 0 ]]; then
        pass_test "Essential files present ($found_count/${#required_files[@]}, missing: ${missing_files[*]})"
    else
        fail_test "Critical files missing (${#missing_files[@]} files not found)"
    fi
}
```

### Docker Environment Testing
```bash
test_docker_functionality() {
    local image="$1"
    local test_command="$2"
    local expected_pattern="$3"
    
    # Verify image exists
    assert_docker_image_exists "$image" "Docker image should be available"
    
    echo "  Running test command in container..."
    
    # Execute test in container with timeout
    local output
    output=$(timeout 60 docker run --rm "$image" bash -c "$test_command" 2>&1)
    local exit_code=$?
    
    # Validate exit code
    if [[ $exit_code -ne 0 ]]; then
        fail_test "Docker command failed (exit $exit_code): $output"
    fi
    
    # Validate output pattern (if provided)
    if [[ -n "$expected_pattern" ]]; then
        assert_contains "$output" "$expected_pattern" "Output should contain expected pattern"
    fi
    
    echo "  ‚úì Docker test completed successfully"
}
```

### Security Validation Pattern
```bash
test_input_security() {
    local validation_func="$1"
    local test_description="$2"
    
    echo "  Testing security: $test_description"
    
    # Valid inputs should pass
    local -a valid_inputs=("valid_input_1" "valid_input_2")
    for input in "${valid_inputs[@]}"; do
        if $validation_func "$input" >/dev/null 2>&1; then
            echo "    ‚úì Valid input accepted: $input"
        else
            fail_test "Valid input rejected: $input"
        fi
    done
    
    # Malicious inputs should be rejected
    local -a malicious_inputs=(
        "../../../etc/passwd"
        "; rm -rf /"
        "\$(whoami)"
        "'; DROP TABLE users; --"
        "\`cat /etc/shadow\`"
    )
    
    for input in "${malicious_inputs[@]}"; do
        if ! $validation_func "$input" >/dev/null 2>&1; then
            echo "    ‚úì Malicious input rejected: $input"
        else
            fail_test "SECURITY VIOLATION: Malicious input accepted: $input"
        fi
    done
    
    echo "  ‚úì Security validation passed"
}
```

### Performance Measurement
```bash
test_performance_benchmark() {
    local operation="$1"
    local max_time_seconds="$2"
    local description="$3"
    
    echo "  Benchmarking: $description"
    
    local start_time
    start_time=$(date +%s.%N)
    
    # Execute operation
    eval "$operation" >/dev/null 2>&1
    local exit_code=$?
    
    local end_time
    end_time=$(date +%s.%N)
    
    local duration
    duration=$(echo "$end_time - $start_time" | bc)
    
    echo "  ‚è±Ô∏è Duration: ${duration}s (limit: ${max_time_seconds}s)"
    
    # Check operation success
    if [[ $exit_code -ne 0 ]]; then
        fail_test "Operation failed with exit code $exit_code"
    fi
    
    # Check performance constraint
    if (( $(echo "$duration <= $max_time_seconds" | bc -l) )); then
        echo "  ‚úì Performance within acceptable limits"
    else
        fail_test "Performance exceeded limit: ${duration}s > ${max_time_seconds}s"
    fi
}
```

---

## Color and Output Standards

### Color Codes
```bash
RED='\033[0;31m'      # Failures, errors
GREEN='\033[0;32m'    # Success, pass
YELLOW='\033[1;33m'   # Warnings, skip
BLUE='\033[0;34m'     # Information, test names
PURPLE='\033[0;35m'   # Special highlights
NC='\033[0m'          # Reset/no color
```

### Output Symbols
```bash
‚úì  # Success indicator
‚úó  # Failure indicator  
‚ö†  # Warning indicator
‚è±Ô∏è  # Performance/timing
üîç # Investigation/analysis
üìä # Metrics/statistics
üéØ # Target/goal
üõ°Ô∏è # Security related
```

### Message Format Standards
```bash
echo "  ‚úì Component validated: $component_name"
echo "  ‚úó Component failed: $component_name ($error_reason)"
echo "  ‚ö† Component warning: $component_name ($warning_reason)"
```

---

## Environment Variables

### Framework Variables
```bash
TEST_ROOT          # Test directory root
PROJECT_ROOT       # Project directory root
TEST_NAME          # Current test name
TEST_PASSED        # Test pass flag
TEST_FAILED        # Test fail flag
```

### Configuration Variables
```bash
TEST_DEBUG=1       # Enable debug output
TEST_VERBOSE=1     # Enable verbose output
TEST_TIMEOUT=60    # Default test timeout
CI=true           # CI environment flag
```

### Project-Specific Variables
```bash
DOCKER_IMAGE="quillkernel-unified"    # Default Docker image
MEMORY_LIMIT_MB=256                   # Hardware memory limit
MEMORY_THRESHOLD_MB=96                # OS memory threshold
```

---

## Test Result Codes

```bash
0   # Test passed successfully
1   # Test failed with errors
2   # Test skipped (dependency not met)
3   # Test timeout
4   # Test setup failure
130 # Test interrupted (SIGINT)
```

---

## Common Test Scenarios

### Boot Script Validation
```bash
test_boot_scripts() {
    local boot_dir="$PROJECT_ROOT/source/scripts/boot"
    local -a required_scripts=("boot-jester.sh" "squireos-boot.sh")
    
    assert_directory_exists "$boot_dir" "Boot scripts directory"
    
    for script in "${required_scripts[@]}"; do
        local script_path="$boot_dir/$script"
        assert_file_exists "$script_path" "Boot script: $script"
        assert_executable "$script_path" "Boot script should be executable: $script"
        
        # Validate script safety
        if grep -q "set -euo pipefail" "$script_path"; then
            echo "  ‚úì Safety headers present: $script"
        else
            fail_test "Missing safety headers in: $script"
        fi
    done
}
```

### Module Compilation Check
```bash
test_module_compilation() {
    local module_dir="$PROJECT_ROOT/source/kernel/src/drivers/squireos"
    local -a required_modules=("squireos_core.c" "jester.c" "typewriter.c" "wisdom.c")
    
    for module in "${required_modules[@]}"; do
        local module_path="$module_dir/$module"
        
        if [[ -f "$module_path" ]]; then
            # Check for valid kernel module structure
            if grep -q "module_init\|MODULE_LICENSE" "$module_path"; then
                echo "  ‚úì Valid kernel module: $module"
            else
                fail_test "Invalid kernel module structure: $module"
            fi
        else
            echo "  ‚ö† Module source not found: $module"
        fi
    done
}
```

### Security Validation
```bash
test_script_security() {
    local script="$1"
    
    echo "  Validating security for: $(basename "$script")"
    
    # Check for dangerous patterns
    local -a dangerous_patterns=(
        "rm -rf /"
        "sudo.*rm"
        "chmod.*777"
        "eval.*\$"
        "exec.*\$"
    )
    
    for pattern in "${dangerous_patterns[@]}"; do
        if grep -q "$pattern" "$script"; then
            fail_test "Dangerous pattern found in $script: $pattern"
        fi
    done
    
    # Check for input validation
    if grep -q "validate_\|check_\|\[\[.*-n.*\]\]" "$script"; then
        echo "  ‚úì Input validation present"
    else
        echo "  ‚ö† Limited input validation detected"
    fi
}
```

---

## Debugging and Troubleshooting

### Debug Mode
```bash
# Enable maximum debugging
export TEST_DEBUG=1
export TEST_VERBOSE=1
set -x

# Run test with trace
bash -x ./tests/unit/memory/test-docker-memory-limit.sh
```

### Common Issues and Solutions

| Issue | Symptom | Solution |
|-------|---------|----------|
| Docker not available | "docker: command not found" | Install Docker, add user to docker group |
| Permission denied | "Permission denied" on test files | `chmod +x tests/**/*.sh` |
| Image not found | "docker: Error response from daemon" | Build required Docker images first |
| Memory test fails | Memory usage exceeds limits | Check for memory leaks, optimize containers |
| Module test fails | "Module not found" | Run `./build_kernel.sh` to compile modules |
| Path validation fails | Security tests reject valid paths | Update path validation rules |

### Test Output Analysis
```bash
# Capture full test output
./tests/run-all-tests.sh 2>&1 | tee test_output.log

# Analyze failures
grep -E "(FAIL|‚úó)" test_output.log

# Check performance
grep -E "(‚è±Ô∏è|Duration)" test_output.log

# Review security issues  
grep -E "(SECURITY|üõ°Ô∏è)" test_output.log
```

---

## Integration with Build System

### Makefile Integration
```makefile
# Test targets
test: test-unit test-integration
test-unit:
	@./tests/run-all-tests.sh
test-integration:
	@./tests/test-high-priority.sh
test-safety:
	@./tests/test-improvements.sh
test-memory:
	@./tests/unit/memory/test-docker-memory-limit.sh
```

### CI/CD Integration
```yaml
# Example GitHub Actions integration
steps:
  - name: Run unit tests
    run: ./tests/run-all-tests.sh
  - name: Run safety tests
    run: ./tests/test-improvements.sh
  - name: Upload test results
    uses: actions/upload-artifact@v2
    with:
      name: test-results
      path: /tmp/test_results/
```

---

*Test Framework Reference v1.0*  
*Quick reference for QuillKernel test development* üïØÔ∏èüìú